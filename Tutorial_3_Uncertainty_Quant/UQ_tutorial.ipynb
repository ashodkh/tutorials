{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a259519",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Tutorial 3: Uncertainty Quantification (UQ)\n",
    "\n",
    "## Mastering Prediction Confidence in Protein Machine Learning\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "1. Assess and improve model calibration using temperature scaling\n",
    "2. Implement heteroscedastic models to capture prediction uncertainty\n",
    "3. Use MC dropout to estimate epistemic uncertainty\n",
    "4. Apply conformal prediction for distribution-free uncertainty intervals\n",
    "5. Distinguish between different types of uncertainty in your predictions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0998fda",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Step 1: Environment Setup\n",
    "\n",
    "### 1.1 Importing Libraries\n",
    "\n",
    "Necessary packages include:\n",
    "- **Transformers & Datasets**: For working with protein language models\n",
    "- **Core ML Libraries**: scikit-learn for utilities, PyTorch for deep learning\n",
    "- **Visualization**: matplotlib and seaborn for plotting\n",
    "- **Data Processing**: pandas and numpy for data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e22a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CORE PYTHON LIBRARIES\n",
    "# ============================================================\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import zipfile\n",
    "import logging\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "from math import ceil\n",
    "\n",
    "# ============================================================\n",
    "# DATA HANDLING & PROCESSING\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ============================================================\n",
    "# MACHINE LEARNING & DEEP LEARNING\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ============================================================\n",
    "# SCIKIT-LEARN UTILITIES\n",
    "# ============================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ============================================================\n",
    "# STATISTICAL & SCIENTIFIC COMPUTING\n",
    "# ============================================================\n",
    "from scipy.special import softmax\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "# ============================================================\n",
    "# UTILITIES\n",
    "# ============================================================\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed29346c",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Step 2: Data Acquisition\n",
    "\n",
    "### 2.1 About the CARE Dataset\n",
    "\n",
    "We'll start with the [**CARE** benchmarking dataset](https://github.com/jsunn-y/CARE/), which includes protein sequences annotated with their EC (Enzyme Commission) numbers.\n",
    "\n",
    "**What are EC Numbers?**\n",
    "- EC numbers classify enzymes based on the reactions they catalyze\n",
    "- Hierarchical system with multiple levels of specificity\n",
    "- Examples:\n",
    "  - **EC1**: Oxidoreductases\n",
    "  - **EC2**: Transferases\n",
    "  - **EC 2.3.1**: Acyltransferases (transferring groups other than amino-acyl groups)\n",
    "\n",
    "### 2.2 Downloading CARE Dataset\n",
    "\n",
    "We'll download the processed CARE datasets from Zenodo if not already available locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7419f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURE DATA PATHS\n",
    "# ============================================================\n",
    "DATA_DIR = Path(\"data/care\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ZIP_PATH = DATA_DIR / \"datasets.zip\"\n",
    "ZENODO_URL = \"https://zenodo.org/record/12195378/files/datasets.zip?download=1\"\n",
    "\n",
    "# ============================================================\n",
    "# DOWNLOAD CARE DATASET FROM ZENODO\n",
    "# ============================================================\n",
    "if not ZIP_PATH.exists():\n",
    "    print(\"ðŸ“¥ Downloading CARE processed datasets from Zenodo...\")\n",
    "    print(\"â³ This may take a minute...\")\n",
    "    \n",
    "    with requests.get(ZENODO_URL, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        total = int(r.headers.get(\"content-length\", 0))\n",
    "        \n",
    "        with open(ZIP_PATH, \"wb\") as f, tqdm(total=total, unit='B', unit_scale=True, desc=\"CARE zip\") as pbar:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "    print(\"âœ“ Download complete!\")\n",
    "else:\n",
    "    print(f\"âœ“ Found existing CARE zip at {ZIP_PATH}\")\n",
    "\n",
    "# ============================================================\n",
    "# EXTRACT DATASET\n",
    "# ============================================================\n",
    "if not (DATA_DIR / \"datasets\").is_dir():\n",
    "    print(\"ðŸ“‚ Extracting CARE datasets...\")\n",
    "    with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
    "        z.extractall(DATA_DIR)\n",
    "    print(f\"âœ“ Extracted CARE datasets to {DATA_DIR}\")\n",
    "else:\n",
    "    print(\"âœ“ CARE datasets already extracted\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Dataset download and extraction complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbd1925",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 3: Data Loading and Preparation\n",
    "\n",
    "### 3.1 Loading Train and Test Datasets\n",
    "\n",
    "We'll load the CARE protein datasets and prepare them for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528809c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD TRAINING DATA\n",
    "# ============================================================\n",
    "train_file = DATA_DIR / \"datasets/splits/task1/protein_train.csv\" \n",
    "train_df = pd.read_csv(train_file, index_col=0)\n",
    "\n",
    "print(\"âœ“ Loaded train dataframe\")\n",
    "print(f\"  Shape: {train_df.shape}\")\n",
    "print(f\"\\nðŸ“‹ First few rows:\")\n",
    "print(train_df.head())\n",
    "\n",
    "# ============================================================\n",
    "# LOAD TEST DATA (MULTIPLE SPLITS)\n",
    "# ============================================================\n",
    "test_file_1 = DATA_DIR / \"datasets/splits/task1/30-50_protein_test.csv\" \n",
    "test_file_2 = DATA_DIR / \"datasets/splits/task1/30_protein_test.csv\" \n",
    "test_file_3 = DATA_DIR / \"datasets/splits/task1/50-70_protein_test.csv\" \n",
    "\n",
    "# Concatenate all test splits\n",
    "test_df = pd.concat([\n",
    "    pd.read_csv(test_file_1),\n",
    "    pd.read_csv(test_file_2),\n",
    "    pd.read_csv(test_file_3)\n",
    "])\n",
    "\n",
    "print(f\"\\nâœ“ Loaded test dataframe\")\n",
    "print(f\"  Shape: {test_df.shape}\")\n",
    "print(f\"\\nðŸ“‹ First few rows:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d9d39e",
   "metadata": {},
   "source": [
    "### 3.2 Preparing Classification Dataset\n",
    "\n",
    "We'll prepare the data for multi-class classification by:\n",
    "1. Selecting top EC1 classes (top level of EC hierarchy)\n",
    "2. Filtering datasets to include only these classes\n",
    "3. Creating label mappings\n",
    "4. Splitting into train/val/test/calibration sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5b6d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURE DATASET PARAMETERS\n",
    "# ============================================================\n",
    "seq_col = \"Sequence\"\n",
    "LABEL_COL = \"EC1\"  # Top level of EC hierarchy only\n",
    "TOP_N = 10  # Number of top classes to keep (will include all 7 EC1 classes)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: SELECT TOP N CLASSES\n",
    "# ============================================================\n",
    "label_counts = train_df[LABEL_COL].value_counts()\n",
    "valid_labels = label_counts.nlargest(TOP_N).index.tolist()\n",
    "\n",
    "print(f\"ðŸ“Š Selected top {len(valid_labels)} EC1 classes:\")\n",
    "for i, label in enumerate(valid_labels, 1):\n",
    "    count = label_counts[label]\n",
    "    print(f\"   {i}. EC{label}: {count} samples\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: FILTER AND MAP LABELS FOR TRAINING DATA\n",
    "# ============================================================\n",
    "train_df = train_df[train_df[LABEL_COL].isin(valid_labels)].copy()\n",
    "\n",
    "# Create label mappings\n",
    "label_to_id = {l: i for i, l in enumerate(valid_labels)}\n",
    "id_to_label = {i: l for l, i in label_to_id.items()}\n",
    "\n",
    "# Add integer label IDs\n",
    "train_df['label_id'] = train_df[LABEL_COL].map(label_to_id)\n",
    "\n",
    "print(f\"\\nâœ“ Filtered training data: {train_df.shape[0]} samples\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: FILTER AND MAP LABELS FOR TEST DATA\n",
    "# ============================================================\n",
    "test_df = test_df[test_df[LABEL_COL].isin(valid_labels)].copy()\n",
    "test_df['label_id'] = test_df[LABEL_COL].map(label_to_id)\n",
    "\n",
    "print(f\"âœ“ Filtered test data: {test_df.shape[0]} samples\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4: CREATE TRAIN/VAL/TEST/CALIBRATION SPLITS\n",
    "# ============================================================\n",
    "# Split training data into train and validation (stratified)\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df, \n",
    "    test_size=0.2, \n",
    "    stratify=train_df['label_id'], \n",
    "    random_state=4242\n",
    ")\n",
    "\n",
    "# Split test data into test and calibration sets (stratified)\n",
    "test_df, test_cal_df = train_test_split(\n",
    "    test_df, \n",
    "    test_size=0.2, \n",
    "    stratify=test_df['label_id'], \n",
    "    random_state=4224\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Dataset Split Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Train:       {train_df.shape[0]:>6} samples\")\n",
    "print(f\"  Validation:  {val_df.shape[0]:>6} samples\")\n",
    "print(f\"  Test:        {test_df.shape[0]:>6} samples\")\n",
    "print(f\"  Calibration: {test_cal_df.shape[0]:>6} samples\")\n",
    "print(f\"  Num Classes: {len(valid_labels):>6}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855856b7",
   "metadata": {},
   "source": [
    "## ðŸ¤– Step 4: Model Setup and Embedding Extraction\n",
    "\n",
    "### 4.1 Loading ESM Protein Language Model\n",
    "\n",
    "We'll use embeddings from the frozen ESM2 base model as features for our classifier. ESM2 is a state-of-the-art protein language model trained on millions of protein sequences.\n",
    "\n",
    "**Architecture Overview:**\n",
    "- **Base Model**: ESM2 (facebook/esm2_t6_8M_UR50D)\n",
    "- **Embedding Strategy**: Extract CLS token embeddings\n",
    "- **Model State**: Frozen (no fine-tuning)\n",
    "- **Precision**: Half precision (float16) for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459bca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURE COMPUTE BACKEND\n",
    "# ============================================================\n",
    "def get_backend():\n",
    "    \"\"\"\n",
    "    Detect and return the available compute backend.\n",
    "    Returns: (device, backend_name, n_gpus)\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        backend = \"cuda\"\n",
    "        n_gpus = torch.cuda.device_count()\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        backend = \"mps\"\n",
    "        n_gpus = 0\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        backend = \"cpu\"\n",
    "        n_gpus = 0\n",
    "    return device, backend, n_gpus\n",
    "\n",
    "DEVICE, BACKEND, N_GPUS = get_backend()\n",
    "\n",
    "print(\"ðŸ–¥ï¸  Compute Configuration:\")\n",
    "print(f\"   Device:  {DEVICE}\")\n",
    "print(f\"   Backend: {BACKEND}\")\n",
    "print(f\"   GPUs:    {N_GPUS}\")\n",
    "\n",
    "# ============================================================\n",
    "# LOAD ESM2 MODEL AND TOKENIZER\n",
    "# ============================================================\n",
    "MODEL_NAME = \"facebook/esm2_t6_8M_UR50D\"  # 8M parameter model\n",
    "# Alternative options:\n",
    "# MODEL_NAME = \"facebook/esm2_t12_35M_UR50D\"  # 35M parameters\n",
    "# MODEL_NAME = \"facebook/esm2_t33_650M_UR50D\"  # 650M parameters\n",
    "\n",
    "print(f\"\\nðŸ§¬ Loading ESM2 model: {MODEL_NAME}\")\n",
    "print(\"   Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=1500, truncation=True)\n",
    "print(\"   âœ“ Tokenizer loaded\")\n",
    "\n",
    "print(\"   Loading model...\")\n",
    "esm_model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "print(\"   âœ“ Model loaded\")\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURE MODEL FOR EMBEDDING EXTRACTION\n",
    "# ============================================================\n",
    "print(\"\\nâš™ï¸  Configuring model:\")\n",
    "\n",
    "# Set to evaluation mode\n",
    "esm_model.eval()\n",
    "print(\"   âœ“ Set to evaluation mode\")\n",
    "\n",
    "# Convert to half precision for efficiency\n",
    "esm_model.half()\n",
    "print(\"   âœ“ Converted to half precision (float16)\")\n",
    "\n",
    "# Freeze all parameters (no training)\n",
    "for p in esm_model.parameters():\n",
    "    p.requires_grad = False\n",
    "print(\"   âœ“ Frozen all parameters\")\n",
    "\n",
    "print(\"\\nâœ“ ESM2 model ready for embedding extraction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678310ca",
   "metadata": {},
   "source": [
    "### 4.2 Defining Embedding Extraction Functions\n",
    "\n",
    "We'll create utility functions to efficiently extract protein sequence embeddings in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c4a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EMBEDDING EXTRACTION FUNCTION\n",
    "# ============================================================\n",
    "def get_esm_embeddings(seqs, model, tokenizer, device, max_batch=256):\n",
    "    \"\"\"\n",
    "    Extract ESM embeddings for a list of protein sequences.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    seqs : list of str\n",
    "        Protein sequences to embed\n",
    "    model : AutoModel\n",
    "        Pre-loaded ESM model\n",
    "    tokenizer : AutoTokenizer\n",
    "        Pre-loaded ESM tokenizer\n",
    "    device : torch.device\n",
    "        Device to run on (CPU/GPU)\n",
    "    max_batch : int\n",
    "        Maximum batch size for processing\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Array of embeddings, shape (n_sequences, embedding_dim)\n",
    "    \"\"\"\n",
    "    embs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(seqs), max_batch), desc=\"Extracting embeddings\"):\n",
    "            # Get batch of sequences\n",
    "            batch = seqs[i:i+max_batch]\n",
    "            \n",
    "            # Tokenize sequences\n",
    "            toks = tokenizer(\n",
    "                batch, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                add_special_tokens=True\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            toks = {k: v.to(device) for k, v in toks.items()}\n",
    "            \n",
    "            # Extract embeddings\n",
    "            out = model(**toks, return_dict=True)\n",
    "            \n",
    "            # Get CLS token embeddings (first token)\n",
    "            cls_emb = out.last_hidden_state[:, 0, :]\n",
    "            \n",
    "            # Store on CPU\n",
    "            embs.append(cls_emb.cpu())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    return torch.cat(embs, dim=0).numpy()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# HELPER FUNCTION: MAINTAIN ORIGINAL ORDER\n",
    "# ============================================================\n",
    "def embed_sequences_in_original_order(seqs, model, tokenizer, device, **kwargs):\n",
    "    \"\"\"\n",
    "    Extract embeddings while maintaining original sequence order.\n",
    "    Sequences are sorted by length for efficiency, then unsorted.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    seqs : list of str\n",
    "        Protein sequences to embed\n",
    "    model, tokenizer, device : \n",
    "        Model components\n",
    "    **kwargs : \n",
    "        Additional arguments for get_esm_embeddings\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Embeddings in original sequence order\n",
    "    \"\"\"\n",
    "    # Track original indices\n",
    "    indexed = list(enumerate(seqs))\n",
    "    \n",
    "    # Sort by sequence length for efficiency\n",
    "    indexed_sorted = sorted(indexed, key=lambda x: len(x[1]))\n",
    "    sorted_indices, sorted_seqs = zip(*indexed_sorted)\n",
    "    \n",
    "    # Get embeddings in sorted order\n",
    "    sorted_embs = get_esm_embeddings(sorted_seqs, model, tokenizer, device, **kwargs)\n",
    "    \n",
    "    # Unsort to original order\n",
    "    sorted_indices = np.array(sorted_indices)\n",
    "    reverse_idx = np.argsort(sorted_indices)\n",
    "    \n",
    "    return sorted_embs[reverse_idx]\n",
    "\n",
    "print(\"âœ“ Embedding extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40854153",
   "metadata": {},
   "source": [
    "### 4.3 Extracting Sequence Embeddings\n",
    "\n",
    "Now we'll extract embeddings for all protein sequences in our train, validation, test, and calibration sets. This process may take a few minutes depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbede2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXTRACT TRAINING SET EMBEDDINGS\n",
    "# ============================================================\n",
    "print(\"ðŸ”„ Extracting train embeddings...\")\n",
    "X_train = embed_sequences_in_original_order(\n",
    "    train_df[seq_col].tolist(), \n",
    "    esm_model, \n",
    "    tokenizer, \n",
    "    DEVICE\n",
    ")\n",
    "y_train = train_df['label_id'].values\n",
    "print(f\"   âœ“ Train embeddings: {X_train.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# EXTRACT VALIDATION SET EMBEDDINGS\n",
    "# ============================================================\n",
    "print(\"\\nðŸ”„ Extracting validation embeddings...\")\n",
    "X_val = embed_sequences_in_original_order(\n",
    "    val_df[seq_col].tolist(), \n",
    "    esm_model, \n",
    "    tokenizer, \n",
    "    DEVICE\n",
    ")\n",
    "y_val = val_df['label_id'].values\n",
    "print(f\"   âœ“ Val embeddings: {X_val.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# EXTRACT TEST SET EMBEDDINGS\n",
    "# ============================================================\n",
    "print(\"\\nðŸ”„ Extracting test embeddings...\")\n",
    "X_test = embed_sequences_in_original_order(\n",
    "    test_df[seq_col].tolist(), \n",
    "    esm_model, \n",
    "    tokenizer, \n",
    "    DEVICE\n",
    ")\n",
    "y_test = test_df['label_id'].values\n",
    "print(f\"   âœ“ Test embeddings: {X_test.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# EXTRACT CALIBRATION SET EMBEDDINGS\n",
    "# ============================================================\n",
    "print(\"\\nðŸ”„ Extracting calibration embeddings...\")\n",
    "X_test_cal = embed_sequences_in_original_order(\n",
    "    test_cal_df[seq_col].tolist(), \n",
    "    esm_model, \n",
    "    tokenizer, \n",
    "    DEVICE\n",
    ")\n",
    "y_test_cal = test_cal_df['label_id'].values\n",
    "print(f\"   âœ“ Calibration embeddings: {X_test_cal.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Embedding Extraction Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Train:       {X_train.shape}\")\n",
    "print(f\"  Validation:  {X_val.shape}\")\n",
    "print(f\"  Test:        {X_test.shape}\")\n",
    "print(f\"  Calibration: {X_test_cal.shape}\")\n",
    "print(f\"  Embedding dimension: {X_train.shape[1]}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d465f1",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Step 5: Building an MLP Classifier\n",
    "\n",
    "### 5.1 Defining the Model Architecture\n",
    "\n",
    "We'll create a simple Multi-Layer Perceptron (MLP) classifier on top of the frozen ESM embeddings. This classifier will predict EC numbers from the protein embeddings.\n",
    "\n",
    "**Architecture:**\n",
    "- Input layer: ESM embedding dimension\n",
    "- Hidden layer: 128 units with ReLU activation\n",
    "- Dropout: 0.01 for regularization\n",
    "- Output layer: Number of EC classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a51d3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEFINE MLP CLASSIFIER ARCHITECTURE\n",
    "# ============================================================\n",
    "class MLPClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple MLP classifier for EC number prediction.\n",
    "    \n",
    "    Architecture:\n",
    "    - Linear layer: in_dim -> hidden_dim\n",
    "    - ReLU activation\n",
    "    - Dropout for regularization\n",
    "    - Linear layer: hidden_dim -> num_classes\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, num_classes, hidden_dim=128, dropout=0.01):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# ============================================================\n",
    "# INITIALIZE MODEL\n",
    "# ============================================================\n",
    "num_classes = len(valid_labels)\n",
    "in_dim = X_train.shape[1]\n",
    "\n",
    "model_clf = MLPClassifier(in_dim, num_classes).to(DEVICE)\n",
    "\n",
    "print(\"ðŸ—ï¸  Model Architecture:\")\n",
    "print(f\"   Input dimension:  {in_dim}\")\n",
    "print(f\"   Hidden dimension: 128\")\n",
    "print(f\"   Output classes:   {num_classes}\")\n",
    "print(f\"   Dropout rate:     0.01\")\n",
    "print(f\"\\nâœ“ Model initialized on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736b3dbf",
   "metadata": {},
   "source": [
    "### 5.2 Configuring Training Setup\n",
    "\n",
    "We'll set up class weights to handle imbalanced data, configure the optimizer and loss function, and prepare data tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93a5d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPUTE CLASS WEIGHTS (FOR IMBALANCED DATA)\n",
    "# ============================================================\n",
    "print(\"âš–ï¸  Computing class weights for balanced training...\")\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(num_classes),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)\n",
    "print(\"   âœ“ Class weights computed\")\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURE OPTIMIZER AND LOSS\n",
    "# ============================================================\n",
    "optimizer = torch.optim.Adam(model_clf.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "print(\"\\nâš™ï¸  Training Configuration:\")\n",
    "print(f\"   Optimizer:    Adam (lr=0.01)\")\n",
    "print(f\"   Loss:         CrossEntropyLoss (weighted)\")\n",
    "print(f\"   Epochs:       500\")\n",
    "\n",
    "# ============================================================\n",
    "# PREPARE DATA TENSORS\n",
    "# ============================================================\n",
    "print(\"\\nðŸ”„ Converting data to PyTorch tensors...\")\n",
    "\n",
    "# Training data\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32).to(DEVICE)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long).to(DEVICE)\n",
    "\n",
    "# Validation data\n",
    "X_val_t = torch.tensor(X_val, dtype=torch.float32).to(DEVICE)\n",
    "y_val_t = torch.tensor(y_val, dtype=torch.long).to(DEVICE)\n",
    "\n",
    "# Test data\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.long).to(DEVICE)\n",
    "\n",
    "# Calibration data\n",
    "X_test_cal_t = torch.tensor(X_test_cal, dtype=torch.float32).to(DEVICE)\n",
    "y_test_cal_t = torch.tensor(y_test_cal, dtype=torch.long).to(DEVICE)\n",
    "\n",
    "print(\"   âœ“ All data tensors prepared and moved to device\")\n",
    "print(\"\\nâœ“ Ready to start training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb6b3c0",
   "metadata": {},
   "source": [
    "### 5.3 Training the Classifier\n",
    "\n",
    "Now we'll train the model for 500 epochs, monitoring both training and validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5655ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================\n",
    "print(\"ðŸš€ Starting training...\\n\")\n",
    "\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    # --------------------------------------------------------\n",
    "    # TRAINING PHASE\n",
    "    # --------------------------------------------------------\n",
    "    model_clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = model_clf(X_train_t)\n",
    "    loss = criterion(logits, y_train_t)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # VALIDATION PHASE\n",
    "    # --------------------------------------------------------\n",
    "    model_clf.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits = model_clf(X_val_t)\n",
    "        val_loss = criterion(val_logits, y_val_t)\n",
    "        val_preds = torch.argmax(val_logits, dim=1)\n",
    "        val_acc = (val_preds == y_val_t).float().mean()\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # LOGGING (every 50 epochs)\n",
    "    # --------------------------------------------------------\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1:03d} | \"\n",
    "              f\"train_loss={loss.item():.4f} | \"\n",
    "              f\"val_loss={val_loss.item():.4f} | \"\n",
    "              f\"val_acc={val_acc.item():.4f}\")\n",
    "\n",
    "print(\"\\nâœ“ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28517ad5",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 6: Model Evaluation and Calibration Analysis\n",
    "\n",
    "### 6.1 Evaluating Test Set Performance\n",
    "\n",
    "Now we'll evaluate our trained classifier on the test set and analyze the probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbfda49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATE ON TEST SET\n",
    "# ============================================================\n",
    "print(\"ðŸ“Š Evaluating model on test set...\\n\")\n",
    "\n",
    "model_clf.eval()\n",
    "\n",
    "# Get predictions for test set\n",
    "with torch.no_grad():\n",
    "    logits_test = model_clf(X_test_t).cpu().numpy()\n",
    "    probs_test = softmax(logits_test, axis=1)\n",
    "\n",
    "# Get predictions for validation set\n",
    "with torch.no_grad():\n",
    "    logits_val = model_clf(X_val_t).cpu().numpy()\n",
    "    probs_val = softmax(logits_val, axis=1)\n",
    "\n",
    "# Get predictions for calibration set\n",
    "with torch.no_grad():\n",
    "    logits_cal = model_clf(X_test_cal_t).cpu().numpy()\n",
    "    probs_cal = softmax(logits_cal, axis=1)\n",
    "\n",
    "# ============================================================\n",
    "# COMPUTE OVERALL METRICS\n",
    "# ============================================================\n",
    "y_pred = probs_test.argmax(1)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "ll = log_loss(y_test, probs_test)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Overall Test Performance:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Accuracy:  {acc:.3f}\")\n",
    "print(f\"  Log Loss:  {ll:.3f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# PER-CLASS ACCURACY\n",
    "# ============================================================\n",
    "print(\"\\nðŸ“‹ Per-Class Performance:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "classes = np.unique(y_test)\n",
    "for c in sorted(classes):\n",
    "    mask = y_test == c\n",
    "    class_acc = (y_pred[mask] == y_test[mask]).mean()\n",
    "    print(f\"  EC{id_to_label[c]} (label {c}): \"\n",
    "          f\"accuracy = {class_acc:.3f} \"\n",
    "          f\"(n={mask.sum():>4})\")\n",
    "\n",
    "print(\"-\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# COMPUTE PREDICTIVE ENTROPY\n",
    "# ============================================================\n",
    "def entropy(p):\n",
    "    \"\"\"\n",
    "    Calculate predictive entropy for probability distributions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    p : numpy.ndarray\n",
    "        Probability distributions, shape (n_samples, n_classes)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Entropy values for each sample\n",
    "    \"\"\"\n",
    "    p = np.clip(p, 1e-9, 1)\n",
    "    return -(p * np.log(p)).sum(axis=1)\n",
    "\n",
    "entropies = entropy(probs_test)\n",
    "\n",
    "print(f\"\\nâœ“ Computed predictive entropy\")\n",
    "print(f\"   Mean entropy: {entropies.mean():.3f}\")\n",
    "print(f\"   Std entropy:  {entropies.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74d7206",
   "metadata": {},
   "source": [
    "### 6.2 Analyzing Calibration\n",
    "\n",
    "We'll examine the relationship between model confidence and actual accuracy by plotting calibration curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86a397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PLOT 1: ACCURACY VS. MAX PROBABILITY\n",
    "# ============================================================\n",
    "print(\"ðŸ“ˆ Creating calibration plots...\\n\")\n",
    "\n",
    "max_prob = probs_test.max(axis=1)\n",
    "bins_prob = np.quantile(max_prob, np.linspace(0, 1, 11))\n",
    "\n",
    "# Compute binned statistics\n",
    "bin_acc_prob = []\n",
    "bin_centers_prob = []\n",
    "bin_errs = []\n",
    "\n",
    "for i in range(len(bins_prob) - 1):\n",
    "    mask = (max_prob >= bins_prob[i]) & (max_prob < bins_prob[i+1])\n",
    "    if mask.sum() > 0:\n",
    "        # Accuracy in this bin\n",
    "        bin_acc_prob.append((y_test[mask] == probs_test[mask].argmax(1)).mean())\n",
    "        # Average probability in this bin\n",
    "        bin_centers_prob.append(max_prob[mask].mean())\n",
    "        # Standard error\n",
    "        bin_errs.append(np.std(y_test[mask] == probs_test[mask].argmax(1)) / np.sqrt(mask.sum()))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.errorbar(bin_centers_prob, bin_acc_prob, yerr=bin_errs, \n",
    "             fmt='o-', capsize=4, label='Model', linewidth=2, markersize=8)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration', linewidth=2)\n",
    "plt.xlabel(\"Predicted Probability\", fontsize=12)\n",
    "plt.ylabel(\"Actual Accuracy\", fontsize=12)\n",
    "plt.title(\"Calibration: Accuracy vs. Max Probability\", fontsize=13)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Plot 1: Accuracy vs. Max Probability\")\n",
    "\n",
    "# ============================================================\n",
    "# PLOT 2: ACCURACY VS. ENTROPY\n",
    "# ============================================================\n",
    "bins_entropy = np.quantile(entropies, np.linspace(0, 1, 11))\n",
    "\n",
    "# Compute binned statistics\n",
    "bin_acc = []\n",
    "bin_centers = []\n",
    "bin_errs = []\n",
    "\n",
    "for i in range(len(bins_entropy) - 1):\n",
    "    mask = (entropies >= bins_entropy[i]) & (entropies < bins_entropy[i+1])\n",
    "    if mask.sum() > 0:\n",
    "        # Accuracy in this bin\n",
    "        bin_acc.append((y_test[mask] == probs_test[mask].argmax(1)).mean())\n",
    "        # Average entropy in this bin\n",
    "        bin_centers.append(entropies[mask].mean())\n",
    "        # Standard error\n",
    "        bin_errs.append(np.std(y_test[mask] == probs_test[mask].argmax(1)) / np.sqrt(mask.sum()))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.errorbar(bin_centers, bin_acc, yerr=bin_errs, \n",
    "             fmt='o-', capsize=4, linewidth=2, markersize=8, color='darkorange')\n",
    "plt.xlabel(\"Predictive Entropy\", fontsize=12)\n",
    "plt.ylabel(\"Actual Accuracy\", fontsize=12)\n",
    "plt.title(\"Calibration: Accuracy vs. Entropy\", fontsize=13)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Plot 2: Accuracy vs. Entropy\")\n",
    "print(\"\\nðŸ’¡ Interpretation:\")\n",
    "print(\"   - Higher entropy indicates more uncertainty\")\n",
    "print(\"   - Lower accuracy at high entropy suggests good uncertainty estimation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e74b849",
   "metadata": {},
   "source": [
    "## ðŸŒ¡ï¸ Step 7: Temperature Scaling for Calibration\n",
    "\n",
    "### 7.1 Implementing Temperature Scaling\n",
    "\n",
    "**Temperature Scaling** is a simple yet effective post-processing technique to improve model calibration. It learns a single scalar parameter (temperature) that rescales the logits before applying softmax.\n",
    "\n",
    "**How it works:**\n",
    "- Original probabilities: `softmax(logits)`\n",
    "- Scaled probabilities: `softmax(logits / T)` where T is the temperature\n",
    "- T > 1: Makes predictions more uncertain (flatter distribution)\n",
    "- T < 1: Makes predictions more confident (sharper distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61184651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEFINE TEMPERATURE SCALER\n",
    "# ============================================================\n",
    "class TemperatureScaler(nn.Module):\n",
    "    \"\"\"\n",
    "    Temperature scaling module for calibration.\n",
    "    Learns a single temperature parameter to rescale logits.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.temperature = nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    def forward(self, logits):\n",
    "        return logits / self.temperature\n",
    "\n",
    "# ============================================================\n",
    "# TEMPERATURE FITTING FUNCTION\n",
    "# ============================================================\n",
    "def fit_temperature(logits, labels):\n",
    "    \"\"\"\n",
    "    Fit optimal temperature on a calibration set.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    logits : numpy.ndarray\n",
    "        Model logits on calibration set\n",
    "    labels : numpy.ndarray\n",
    "        True labels\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    TemperatureScaler\n",
    "        Fitted temperature scaler\n",
    "    \"\"\"\n",
    "    model = TemperatureScaler()\n",
    "    optimizer = torch.optim.LBFGS([model.temperature], lr=0.01, max_iter=500)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    logits_t = torch.tensor(logits, dtype=torch.float32)\n",
    "    labels_t = torch.tensor(labels, dtype=torch.long)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Optimization closure\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(logits_t), labels_t)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    # Optimize\n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ============================================================\n",
    "# FIT TEMPERATURE ON CALIBRATION SET\n",
    "# ============================================================\n",
    "print(\"ðŸŒ¡ï¸  Fitting temperature scaler...\\n\")\n",
    "\n",
    "scaler = fit_temperature(logits_cal, y_test_cal)\n",
    "\n",
    "# Apply temperature scaling to test logits\n",
    "scaled_logits = scaler(torch.tensor(logits_test, dtype=torch.float32)).detach().numpy()\n",
    "scaled_probs = softmax(scaled_logits, axis=1)\n",
    "\n",
    "print(f\"âœ“ Temperature fitted: T = {scaler.temperature.item():.4f}\")\n",
    "print(f\"\\nðŸ’¡ Interpretation:\")\n",
    "if scaler.temperature.item() > 1:\n",
    "    print(f\"   T > 1: Model is overconfident, temperature makes it more uncertain\")\n",
    "elif scaler.temperature.item() < 1:\n",
    "    print(f\"   T < 1: Model is underconfident, temperature makes it more confident\")\n",
    "else:\n",
    "    print(f\"   T â‰ˆ 1: Model is well-calibrated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d5dcbe",
   "metadata": {},
   "source": [
    "### 7.2 Reliability Diagrams\n",
    "\n",
    "We'll create reliability diagrams to visualize calibration before and after temperature scaling. A perfectly calibrated model should align with the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb42b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RELIABILITY DIAGRAM FUNCTION\n",
    "# ============================================================\n",
    "def reliability_diagram(probs, y_true, n_bins=10):\n",
    "    \"\"\"\n",
    "    Compute reliability diagram statistics and ECE (Expected Calibration Error).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    probs : numpy.ndarray\n",
    "        Predicted probabilities, shape (n_samples, n_classes)\n",
    "    y_true : numpy.ndarray\n",
    "        True labels\n",
    "    n_bins : int\n",
    "        Number of bins for calibration\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bins, bin_acc, bin_conf, ece : tuple\n",
    "        Bin edges, accuracies, confidences, and ECE score\n",
    "    \"\"\"\n",
    "    # Get confidence (max probability) and predictions\n",
    "    conf = probs.max(axis=1)\n",
    "    preds = probs.argmax(axis=1)\n",
    "    accs = preds == y_true\n",
    "    \n",
    "    # Create bins\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    binids = np.digitize(conf, bins) - 1\n",
    "    \n",
    "    # Initialize arrays\n",
    "    bin_acc = np.zeros(n_bins)\n",
    "    bin_conf = np.zeros(n_bins)\n",
    "    bin_count = np.zeros(n_bins)\n",
    "    \n",
    "    # Compute statistics for each bin\n",
    "    for b in range(n_bins):\n",
    "        mask = binids == b\n",
    "        if mask.sum() > 0:\n",
    "            bin_acc[b] = accs[mask].mean()\n",
    "            bin_conf[b] = conf[mask].mean()\n",
    "            bin_count[b] = mask.sum()\n",
    "        else:\n",
    "            # Mark empty bins\n",
    "            bin_acc[b] = np.nan\n",
    "            bin_conf[b] = np.nan\n",
    "    \n",
    "    # Remove empty bins\n",
    "    mask_valid = ~np.isnan(bin_acc)\n",
    "    bin_acc = bin_acc[mask_valid]\n",
    "    bin_conf = bin_conf[mask_valid]\n",
    "    bin_count = bin_count[mask_valid]\n",
    "    \n",
    "    # Sort by confidence to avoid wrap-around in plots\n",
    "    sort_idx = np.argsort(bin_conf)\n",
    "    bin_acc = bin_acc[sort_idx]\n",
    "    bin_conf = bin_conf[sort_idx]\n",
    "    bin_count = bin_count[sort_idx]\n",
    "    \n",
    "    # Compute Expected Calibration Error (ECE)\n",
    "    ece = np.sum(bin_count * np.abs(bin_acc - bin_conf)) / len(y_true)\n",
    "    \n",
    "    return bins, bin_acc, bin_conf, ece\n",
    "\n",
    "# ============================================================\n",
    "# COMPUTE RELIABILITY DIAGRAMS\n",
    "# ============================================================\n",
    "print(\"\\nðŸ“Š Computing reliability diagrams...\\n\")\n",
    "\n",
    "# Before scaling\n",
    "bins, bin_acc_r, bin_conf_r, ece = reliability_diagram(probs_test, y_test, n_bins=10)\n",
    "\n",
    "# After scaling\n",
    "bins, bin_acc_r_s, bin_conf_r_s, ece_scaled = reliability_diagram(scaled_probs, y_test, n_bins=10)\n",
    "\n",
    "# ============================================================\n",
    "# PRINT RESULTS\n",
    "# ============================================================\n",
    "print(\"=\"*60)\n",
    "print(\"Calibration Metrics:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Temperature:           {scaler.temperature.item():.4f}\")\n",
    "print(f\"  ECE (pre-scaling):     {ece:.4f}\")\n",
    "print(f\"  ECE (post-scaling):    {ece_scaled:.4f}\")\n",
    "print(f\"  Improvement:           {((ece - ece_scaled) / ece * 100):.1f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# PLOT COMPARISON\n",
    "# ============================================================\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration', linewidth=2)\n",
    "plt.plot(bin_conf_r, bin_acc_r, marker='o', linewidth=2, markersize=8,\n",
    "         label=f'Before scaling (ECE={ece:.3f})', color='coral')\n",
    "plt.plot(bin_conf_r_s, bin_acc_r_s, marker='s', linewidth=2, markersize=8,\n",
    "         label=f'After scaling (ECE={ece_scaled:.3f})', color='royalblue')\n",
    "plt.xlabel('Confidence', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.title('Reliability Diagram: Pre & Post Temperature Scaling', fontsize=13)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Temperature scaling improves calibration!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a2f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CLEAN UP CLASSIFICATION TASK VARIABLES\n",
    "# ============================================================\n",
    "print(\"\\nðŸ§¹ Cleaning up classification task variables...\")\n",
    "\n",
    "# Delete classifier model and training tensors\n",
    "del model_clf\n",
    "del X_train_t, y_train_t\n",
    "del X_val_t, y_val_t\n",
    "del X_test_t, y_test_t\n",
    "del X_test_cal_t, y_test_cal_t\n",
    "\n",
    "# Delete classification outputs\n",
    "del logits_test, probs_test, logits_cal\n",
    "\n",
    "# Delete temperature scaler\n",
    "del scaler\n",
    "\n",
    "# Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "elif hasattr(torch, 'mps') and torch.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "    \n",
    "print(\"âœ“ Classification variables removed from memory\")\n",
    "print(\"  Moving to regression task...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71b1a97",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”¬ Step 8: Regression Task - Protein Melting Temperature\n",
    "\n",
    "### 8.1 About the FLIP Meltome Dataset\n",
    "\n",
    "We'll now explore uncertainty quantification for **regression tasks** using the [FLIP benchmark](https://github.com/J-SNACKKB/FLIP/tree/main), which includes protein melting temperature data.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Task**: Predict melting temperature (Tm) of proteins\n",
    "- **OOD Detection**: Create an out-of-distribution test set via clustering\n",
    "- **Data Transform**: Apply power transform to handle skewed target distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c152d0b7",
   "metadata": {},
   "source": [
    "### 8.2 Loading and Preparing Meltome Data\n",
    "\n",
    "We'll download the dataset, split it appropriately, and create an OOD test set using clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dfc64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURE DATA PATHS\n",
    "# ============================================================\n",
    "DATA_DIR = Path(\"data/meltome\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ZIP_PATH = DATA_DIR / \"splits.zip\"\n",
    "DOWNLOAD_URL = \"https://github.com/J-SNACKKB/FLIP/raw/refs/heads/main/splits/meltome/splits.zip?download=1\"\n",
    "\n",
    "# ============================================================\n",
    "# DOWNLOAD MELTOME DATASET\n",
    "# ============================================================\n",
    "if not ZIP_PATH.exists():\n",
    "    print(\"ðŸ“¥ Downloading meltome dataset from GitHub...\")\n",
    "    print(\"â³ This may take a minute...\")\n",
    "    \n",
    "    with requests.get(DOWNLOAD_URL, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        total = int(r.headers.get(\"content-length\", 0))\n",
    "        \n",
    "        with open(ZIP_PATH, \"wb\") as f, tqdm(total=total, unit='B', unit_scale=True, desc=\"Meltome zip\") as pbar:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "    print(\"âœ“ Download complete!\")\n",
    "else:\n",
    "    print(f\"âœ“ Found existing meltome zip at {ZIP_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4983baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# EXTRACT DATASET\n",
    "# ============================================================\n",
    "if not (DATA_DIR / \"splits\").is_dir():\n",
    "    print(\"ðŸ“‚ Extracting meltome datasets...\")\n",
    "    with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
    "        z.extractall(DATA_DIR)\n",
    "    print(f\"âœ“ Extracted to {DATA_DIR}\")\n",
    "else:\n",
    "    print(\"âœ“ Meltome datasets already extracted\")\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA SPLITS\n",
    "# ============================================================\n",
    "print(\"\\nðŸ“Š Loading data splits...\")\n",
    "\n",
    "df = pd.read_csv(DATA_DIR / \"splits/mixed_split.csv\")\n",
    "\n",
    "# Create train/val/test splits\n",
    "train_df = df[(df['set'] == 'train') & ~(df['validation'] == 1)][['sequence', 'target']]\n",
    "val_df = df[(df['set'] == 'train') & df['validation']][['sequence', 'target']]\n",
    "test_df = df[(df['set'] == 'test')][['sequence', 'target']]\n",
    "\n",
    "print(f\"âœ“ Train: {train_df.shape[0]} samples\")\n",
    "print(f\"âœ“ Val:   {val_df.shape[0]} samples\")\n",
    "print(f\"âœ“ Test:  {test_df.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a536903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# EXTRACT EMBEDDINGS\n",
    "# ============================================================\n",
    "seq_col = \"sequence\"\n",
    "\n",
    "print(\"\\nðŸ”„ Extracting embeddings for regression task...\")\n",
    "print(\"   (Using smaller batch size for longer sequences)\\n\")\n",
    "\n",
    "print(\"Extracting train embeddings...\")\n",
    "X_train = embed_sequences_in_original_order(\n",
    "    train_df[seq_col].tolist(), \n",
    "    esm_model, \n",
    "    tokenizer, \n",
    "    DEVICE, \n",
    "    max_batch=64\n",
    ")\n",
    "\n",
    "print(\"Extracting val embeddings...\")\n",
    "X_val = embed_sequences_in_original_order(\n",
    "    val_df[seq_col].tolist(), \n",
    "    esm_model, \n",
    "    tokenizer, \n",
    "    DEVICE, \n",
    "    max_batch=128\n",
    ")\n",
    "\n",
    "print(\"Extracting test embeddings...\")\n",
    "X_test = embed_sequences_in_original_order(\n",
    "    test_df[seq_col].tolist(), \n",
    "    esm_model, \n",
    "    tokenizer, \n",
    "    DEVICE, \n",
    "    max_batch=128\n",
    ")\n",
    "\n",
    "# Prepare targets\n",
    "y_train = train_df[\"target\"].to_numpy().reshape(-1, 1)\n",
    "y_val = val_df[\"target\"].to_numpy().reshape(-1, 1)\n",
    "y_test = test_df[\"target\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "print(\"\\nâœ“ Embeddings extracted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4897f465",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# CREATE OOD TEST SET VIA CLUSTERING\n",
    "# ============================================================\n",
    "print(\"\\nðŸŽ¯ Creating OOD test set via clustering...\")\n",
    "\n",
    "n_clusters = 20\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=4252)\n",
    "cluster_labels = kmeans.fit_predict(X_train)\n",
    "\n",
    "# Find cluster most distant from centroid mean (most OOD)\n",
    "cluster_sizes = np.bincount(cluster_labels)\n",
    "ood_cluster = np.argmax(\n",
    "    np.linalg.norm(kmeans.cluster_centers_ - kmeans.cluster_centers_.mean(0), axis=1)\n",
    ")\n",
    "\n",
    "print(f\"   Selected OOD cluster: {ood_cluster}\")\n",
    "print(f\"   OOD cluster size: {cluster_sizes[ood_cluster]}\")\n",
    "\n",
    "# Split training data into in-distribution and OOD\n",
    "in_mask = cluster_labels != ood_cluster\n",
    "ood_mask = cluster_labels == ood_cluster\n",
    "\n",
    "X_ood = X_train[ood_mask]\n",
    "y_ood = y_train[ood_mask]\n",
    "X_train = X_train[in_mask]\n",
    "y_train = y_train[in_mask]\n",
    "\n",
    "print(f\"\\nâœ“ Split complete:\")\n",
    "print(f\"   In-Distribution Train: {X_train.shape[0]} samples\")\n",
    "print(f\"   OOD Test:              {X_ood.shape[0]} samples\")\n",
    "\n",
    "# ============================================================\n",
    "# APPLY POWER TRANSFORM\n",
    "# ============================================================\n",
    "print(\"\\nâš¡ Applying power transform to targets...\")\n",
    "\n",
    "powt = PowerTransformer(method=\"yeo-johnson\", standardize=True)\n",
    "y_train = powt.fit_transform(y_train).ravel()\n",
    "y_val = powt.transform(y_val).ravel()\n",
    "y_test = powt.transform(y_test).ravel()\n",
    "y_ood = powt.transform(y_ood).ravel()\n",
    "\n",
    "print(\"   âœ“ Power transform applied (helps with skewed distributions)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e1611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# CREATE PYTORCH TENSORS\n",
    "# ============================================================\n",
    "print(\"\\nðŸ”„ Creating PyTorch tensors...\")\n",
    "\n",
    "train_X = torch.tensor(X_train, dtype=torch.float32).to(DEVICE)\n",
    "train_y = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(DEVICE)\n",
    "\n",
    "val_X = torch.tensor(X_val, dtype=torch.float32).to(DEVICE)\n",
    "val_y = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(DEVICE)\n",
    "\n",
    "test_X = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n",
    "test_y = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1).to(DEVICE)\n",
    "\n",
    "ood_X = torch.tensor(X_ood, dtype=torch.float32).to(DEVICE)\n",
    "ood_y = torch.tensor(y_ood, dtype=torch.float32).unsqueeze(1).to(DEVICE)\n",
    "\n",
    "print(\"   âœ“ All tensors ready on device\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Data Preparation Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Train (ID):  {train_X.shape}\")\n",
    "print(f\"  Validation:  {val_X.shape}\")\n",
    "print(f\"  Test (ID):   {test_X.shape}\")\n",
    "print(f\"  Test (OOD):  {ood_X.shape}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1692dfa",
   "metadata": {},
   "source": [
    "## ðŸŽ² Step 9: Heteroscedastic Model with MC Dropout\n",
    "\n",
    "### 9.1 Building a Heteroscedastic MLP\n",
    "\n",
    "We'll train a regression model that predicts **both mean and variance** using a **Gaussian Negative Log-Likelihood (NLL)** loss. This captures **aleatoric uncertainty** (data noise).\n",
    "\n",
    "**Model Architecture:**\n",
    "- Shared layers with ReLU and Dropout\n",
    "- Two heads: one for mean (Î¼), one for log-variance (log ÏƒÂ²)\n",
    "- Training with Gaussian NLL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f5893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEFINE HETEROSCEDASTIC MLP\n",
    "# ============================================================\n",
    "class HeteroMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Heteroscedastic MLP that predicts both mean and variance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    d_in : int\n",
    "        Input dimension\n",
    "    h : int\n",
    "        Hidden layer size\n",
    "    p : float\n",
    "        Dropout probability\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in, h=256, p=0.2):\n",
    "        super().__init__()\n",
    "        # Shared feature extractor\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(d_in, h), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(h, h//2), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(p),\n",
    "        )\n",
    "        # Mean head\n",
    "        self.mu = nn.Linear(h//2, 1)\n",
    "        # Log-variance head (outputs log ÏƒÂ²(x))\n",
    "        self.logvar = nn.Linear(h//2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.f(x)\n",
    "        return self.mu(h), self.logvar(h)\n",
    "\n",
    "# ============================================================\n",
    "# DEFINE GAUSSIAN NLL LOSS\n",
    "# ============================================================\n",
    "def gaussian_nll(mu, logvar, y):\n",
    "    \"\"\"\n",
    "    Gaussian negative log-likelihood loss.\n",
    "    \n",
    "    L = 0.5 * (exp(-logvar) * (y - mu)Â² + logvar)\n",
    "    \n",
    "    This encourages the model to:\n",
    "    - Minimize prediction error (y - mu)Â²\n",
    "    - Learn appropriate uncertainty (logvar)\n",
    "    \"\"\"\n",
    "    inv_var = torch.exp(-logvar)\n",
    "    return 0.5 * (inv_var * (y - mu)**2 + logvar).mean()\n",
    "\n",
    "# ============================================================\n",
    "# INITIALIZE MODEL AND OPTIMIZER\n",
    "# ============================================================\n",
    "input_dim = train_X.shape[1]\n",
    "num_train = train_X.shape[0]\n",
    "\n",
    "model = HeteroMLP(input_dim).to(DEVICE)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "crit = gaussian_nll\n",
    "\n",
    "print(\"ðŸ—ï¸  Heteroscedastic Model Configuration:\")\n",
    "print(f\"   Input dimension:    {input_dim}\")\n",
    "print(f\"   Hidden dimension:   256 â†’ 128\")\n",
    "print(f\"   Output:             Mean + Log-Variance\")\n",
    "print(f\"   Dropout:            0.2\")\n",
    "print(f\"   Optimizer:          Adam (lr=1e-4, wd=1e-5)\")\n",
    "print(f\"   Loss:               Gaussian NLL\")\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ============================================================\n",
    "batch_size = 1024\n",
    "num_epochs = 500\n",
    "patience = 30\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "print(f\"\\nâš™ï¸  Training Configuration:\")\n",
    "print(f\"   Batch size:         {batch_size}\")\n",
    "print(f\"   Max epochs:         {num_epochs}\")\n",
    "print(f\"   Early stopping:     {patience} epochs\")\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================\n",
    "print(\"\\nðŸš€ Starting training...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    perm = torch.randperm(num_train, device=DEVICE)\n",
    "    total_loss = 0.0\n",
    "    logvar_vals = []\n",
    "    \n",
    "    for i in range(0, num_train, batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        xb, yb = train_X[idx], train_y[idx]\n",
    "        \n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        mu, logvar = model(xb)\n",
    "        loss = crit(mu, logvar, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        logvar_vals.append(logvar.mean().item())\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mu_val, lv_val = model(val_X)\n",
    "        val_loss = crit(mu_val, lv_val, val_y).item()\n",
    "        val_logvar = lv_val.mean().item()\n",
    "\n",
    "    train_loss = total_loss / num_train\n",
    "    avg_logvar = np.mean(logvar_vals)\n",
    "\n",
    "    # Logging\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:03d}: \"\n",
    "              f\"train_loss={train_loss:.5f}, val_loss={val_loss:.5f}, \"\n",
    "              f\"avg_logvar={avg_logvar:.3f}, val_logvar={val_logvar:.3f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"\\nâ¹ï¸  Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(\"\\nâœ“ Training complete!\")\n",
    "print(f\"âœ“ Best validation loss: {best_val_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74df9ee1",
   "metadata": {},
   "source": [
    "### 9.2 Monte Carlo Dropout for Epistemic Uncertainty\n",
    "\n",
    "**MC Dropout** keeps dropout active during inference and samples multiple predictions per input. This allows us to estimate:\n",
    "\n",
    "- **Epistemic Uncertainty**: Variance of predicted means (model uncertainty)\n",
    "- **Aleatoric Uncertainty**: Mean of predicted variances (data noise)\n",
    "- **Total Uncertainty**: Sum of both\n",
    "\n",
    "We'll verify calibration by checking coverage at 68% (1Ïƒ) and 95% (1.96Ïƒ) confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2fd4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MC DROPOUT EVALUATION FUNCTION\n",
    "# ============================================================\n",
    "def mc_dropout_eval_hetero(model, X, n=100):\n",
    "    \"\"\"\n",
    "    Perform MC Dropout evaluation on heteroscedastic model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : HeteroMLP\n",
    "        Trained heteroscedastic model\n",
    "    X : torch.Tensor\n",
    "        Input data\n",
    "    n : int\n",
    "        Number of MC samples\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    mus, vars_ : tuple of numpy.ndarray\n",
    "        Arrays of predicted means and variances\n",
    "        Shape: (n_samples, n_datapoints)\n",
    "    \"\"\"\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "    model.train()  # Keep dropout active!\n",
    "    \n",
    "    mus, vars_ = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(n):\n",
    "            mu_s, logv_s = model(X)\n",
    "            mus.append(mu_s.squeeze(1).cpu().numpy())\n",
    "            vars_.append(torch.exp(logv_s).squeeze(1).cpu().numpy())\n",
    "    \n",
    "    mus = np.stack(mus)\n",
    "    vars_ = np.stack(vars_)\n",
    "    \n",
    "    return mus, vars_\n",
    "\n",
    "# ============================================================\n",
    "# PERFORM MC DROPOUT EVALUATION\n",
    "# ============================================================\n",
    "print(\"ðŸŽ² Performing MC Dropout evaluation (100 samples)...\\n\")\n",
    "\n",
    "mus, vars_ = mc_dropout_eval_hetero(model, test_X, n=100)\n",
    "\n",
    "# Compute mean prediction and uncertainties\n",
    "mu_mean = mus.mean(0)\n",
    "epistemic = mus.var(0)          # Variance of means\n",
    "aleatoric = vars_.mean(0)       # Mean of variances\n",
    "total_var = epistemic + aleatoric\n",
    "total_std = np.sqrt(total_var)\n",
    "\n",
    "print(\"âœ“ MC Dropout evaluation complete\")\n",
    "\n",
    "# ============================================================\n",
    "# COMPUTE COVERAGE\n",
    "# ============================================================\n",
    "print(\"\\nðŸ“Š Empirical Coverage Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_test_true = test_y.cpu().numpy().ravel()\n",
    "\n",
    "for z, name in [(1.0, \"68%\"), (1.96, \"95%\")]:\n",
    "    covered = (np.abs(mu_mean - y_test_true) <= z * total_std).mean()\n",
    "    print(f\"  {name} confidence interval (Â±{z}Ïƒ): {covered:.3f} coverage\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸ’¡ Interpretation:\")\n",
    "print(\"   - Good calibration: coverage should match confidence level\")\n",
    "print(\"   - 68% interval should cover ~68% of test points\")\n",
    "print(\"   - 95% interval should cover ~95% of test points\")\n",
    "\n",
    "# ============================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================\n",
    "print(f\"\\nðŸ“ˆ Uncertainty Statistics:\")\n",
    "print(f\"   Mean epistemic uncertainty: {np.sqrt(epistemic).mean():.4f}\")\n",
    "print(f\"   Mean aleatoric uncertainty: {np.sqrt(aleatoric).mean():.4f}\")\n",
    "print(f\"   Mean total uncertainty:     {total_std.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa55d2",
   "metadata": {},
   "source": [
    "### 9.3 Comparing Epistemic vs Aleatoric Uncertainty\n",
    "\n",
    "We'll visualize and compare the two types of uncertainty across in-distribution and OOD samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022df260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPUTE UNCERTAINTIES FOR BOTH SPLITS\n",
    "# ============================================================\n",
    "print(\"ðŸ”„ Computing uncertainties for ID and OOD splits...\\n\")\n",
    "\n",
    "mus_test, vars_test = mc_dropout_eval_hetero(model, test_X, n=100)\n",
    "mus_ood, vars_ood = mc_dropout_eval_hetero(model, ood_X, n=100)\n",
    "\n",
    "# Epistemic = variance of means across MC samples\n",
    "epistemic_test = np.var(mus_test, axis=0, ddof=1)\n",
    "epistemic_ood = np.var(mus_ood, axis=0, ddof=1)\n",
    "\n",
    "# Aleatoric = mean of predicted variances\n",
    "aleatoric_test = np.mean(vars_test, axis=0)\n",
    "aleatoric_ood = np.mean(vars_ood, axis=0)\n",
    "\n",
    "# Convert to standard deviations (more interpretable)\n",
    "epistemic_test_std = np.sqrt(epistemic_test)\n",
    "epistemic_ood_std = np.sqrt(epistemic_ood)\n",
    "aleatoric_test_std = np.sqrt(aleatoric_test)\n",
    "aleatoric_ood_std = np.sqrt(aleatoric_ood)\n",
    "\n",
    "print(\"âœ“ Uncertainties computed for both splits\")\n",
    "\n",
    "# ============================================================\n",
    "# PLOT 1: EPISTEMIC VS ALEATORIC SCATTER\n",
    "# ============================================================\n",
    "print(\"\\nðŸ“Š Creating uncertainty comparison plots...\\n\")\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "\n",
    "plt.scatter(\n",
    "    epistemic_test_std, aleatoric_test_std,\n",
    "    alpha=0.5, s=20, label=\"In-Distribution\", color=\"royalblue\"\n",
    ")\n",
    "plt.scatter(\n",
    "    epistemic_ood_std, aleatoric_ood_std,\n",
    "    alpha=0.5, s=20, label=\"Out-of-Distribution\", color=\"darkorange\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Epistemic Uncertainty (std)\", fontsize=12)\n",
    "plt.ylabel(\"Aleatoric Uncertainty (std)\", fontsize=12)\n",
    "plt.title(\"Epistemic vs Aleatoric Uncertainty\\nIn-Distribution vs OOD\", fontsize=13)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Plot 1: Epistemic vs Aleatoric scatter\")\n",
    "\n",
    "# ============================================================\n",
    "# PLOT 2: BOXPLOT COMPARISON\n",
    "# ============================================================\n",
    "# Prepare data for boxplot\n",
    "in_df = pd.DataFrame({\n",
    "    \"epistemic\": epistemic_test_std,\n",
    "    \"aleatoric\": aleatoric_test_std,\n",
    "})\n",
    "in_df[\"split\"] = \"In-Distribution\"\n",
    "\n",
    "ood_df = pd.DataFrame({\n",
    "    \"epistemic\": epistemic_ood_std,\n",
    "    \"aleatoric\": aleatoric_ood_std,\n",
    "})\n",
    "ood_df[\"split\"] = \"OOD\"\n",
    "\n",
    "unc_df = pd.concat([in_df, ood_df], ignore_index=True)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(data=unc_df, x=\"split\", y=\"epistemic\", hue=\"split\", palette=[\"royalblue\", \"darkorange\"])\n",
    "plt.title(\"Epistemic Uncertainty: In-Distribution vs OOD\", fontsize=13)\n",
    "plt.ylabel(\"Epistemic Uncertainty (std)\", fontsize=12)\n",
    "plt.xlabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Plot 2: Epistemic uncertainty comparison\")\n",
    "\n",
    "# ============================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Uncertainty Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"In-Distribution (Test):\")\n",
    "print(f\"  Epistemic: {epistemic_test_std.mean():.4f} Â± {epistemic_test_std.std():.4f}\")\n",
    "print(f\"  Aleatoric: {aleatoric_test_std.mean():.4f} Â± {aleatoric_test_std.std():.4f}\")\n",
    "print(f\"\\nOut-of-Distribution:\")\n",
    "print(f\"  Epistemic: {epistemic_ood_std.mean():.4f} Â± {epistemic_ood_std.std():.4f}\")\n",
    "print(f\"  Aleatoric: {aleatoric_ood_std.mean():.4f} Â± {aleatoric_ood_std.std():.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insights:\")\n",
    "print(\"   - OOD samples typically show HIGHER epistemic uncertainty\")\n",
    "print(\"   - Aleatoric uncertainty reflects inherent data noise\")\n",
    "print(\"   - Model is more uncertain about unfamiliar protein regions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf749328",
   "metadata": {},
   "source": [
    "### 9.4 Uncertainty vs Prediction Accuracy\n",
    "\n",
    "We'll visualize how uncertainty correlates with prediction accuracy using colored scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cdc785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PREPARE DATA FOR VISUALIZATION\n",
    "# ============================================================\n",
    "print(\"ðŸŽ¨ Creating uncertainty vs accuracy visualizations...\\n\")\n",
    "\n",
    "# Re-compute predictions and uncertainties\n",
    "mus_test, vars_test = mc_dropout_eval_hetero(model, test_X, n=100)\n",
    "mus_ood, vars_ood = mc_dropout_eval_hetero(model, ood_X, n=100)\n",
    "\n",
    "mu_mean_test = mus_test.mean(axis=0)\n",
    "mu_mean_ood = mus_ood.mean(axis=0)\n",
    "\n",
    "epistemic_test = np.var(mus_test, axis=0)\n",
    "epistemic_ood = np.var(mus_ood, axis=0)\n",
    "\n",
    "aleatoric_test = np.mean(vars_test, axis=0)\n",
    "aleatoric_ood = np.mean(vars_ood, axis=0)\n",
    "\n",
    "epistemic_test_std = np.sqrt(epistemic_test)\n",
    "epistemic_ood_std = np.sqrt(epistemic_ood)\n",
    "\n",
    "aleatoric_test_std = np.sqrt(aleatoric_test)\n",
    "aleatoric_ood_std = np.sqrt(aleatoric_ood)\n",
    "\n",
    "y_test_true = test_y.cpu().numpy().ravel()\n",
    "y_ood_true = ood_y.cpu().numpy().ravel()\n",
    "\n",
    "# ============================================================\n",
    "# PLOTTING FUNCTION\n",
    "# ============================================================\n",
    "def _plot_uncertainty_pair(y_test_true, mu_mean_test, unc_test_std,\n",
    "                           y_ood_true, mu_mean_ood, unc_ood_std,\n",
    "                           unc_label=\"Epistemic\"):\n",
    "    \"\"\"\n",
    "    Create side-by-side plots showing true vs predicted values\n",
    "    colored by uncertainty level.\n",
    "    \"\"\"\n",
    "    # Compute shared color scale\n",
    "    vmin = min(unc_test_std.min(), unc_ood_std.min())\n",
    "    vmax = max(unc_test_std.max(), unc_ood_std.max())\n",
    "    \n",
    "    # Compute shared axis limits\n",
    "    all_true = np.concatenate([y_test_true, y_ood_true])\n",
    "    all_pred = np.concatenate([mu_mean_test, mu_mean_ood])\n",
    "    lims = [min(all_true.min(), all_pred.min()), \n",
    "            max(all_true.max(), all_pred.max())]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharex=True, sharey=True)\n",
    "\n",
    "    splits = [\n",
    "        (y_test_true, mu_mean_test, unc_test_std, \"In-Distribution\"),\n",
    "        (y_ood_true, mu_mean_ood, unc_ood_std, \"Out-of-Distribution\")\n",
    "    ]\n",
    "\n",
    "    sc = None\n",
    "    for ax, (y_true, mu_mean, unc_std, title) in zip(axes, splits):\n",
    "        sc = ax.scatter(\n",
    "            y_true, mu_mean,\n",
    "            c=unc_std,\n",
    "            cmap=\"viridis\",\n",
    "            vmin=vmin, vmax=vmax,\n",
    "            s=20, alpha=0.7, edgecolors=\"none\"\n",
    "        )\n",
    "        # Perfect prediction line\n",
    "        ax.plot(lims, lims, \"r--\", lw=2, label=\"Perfect prediction\")\n",
    "        ax.set_xlim(lims)\n",
    "        ax.set_ylim(lims)\n",
    "        ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "        ax.set_title(title, fontsize=13)\n",
    "        ax.set_xlabel(\"True Target\", fontsize=11)\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.legend()\n",
    "\n",
    "    axes[0].set_ylabel(\"Predicted Mean\", fontsize=11)\n",
    "\n",
    "    # Shared colorbar\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "    cbar = fig.colorbar(sc, cax=cbar_ax)\n",
    "    cbar.set_label(f\"{unc_label} Uncertainty (std)\", rotation=270, labelpad=20, fontsize=11)\n",
    "\n",
    "    plt.suptitle(f\"True vs Predicted (color = {unc_label} uncertainty)\", fontsize=14, y=0.98)\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# PLOT EPISTEMIC UNCERTAINTY\n",
    "# ============================================================\n",
    "print(\"Creating epistemic uncertainty visualization...\")\n",
    "_plot_uncertainty_pair(\n",
    "    y_test_true, mu_mean_test, epistemic_test_std,\n",
    "    y_ood_true, mu_mean_ood, epistemic_ood_std,\n",
    "    unc_label=\"Epistemic\"\n",
    ")\n",
    "print(\"âœ“ Epistemic uncertainty plot created\")\n",
    "\n",
    "# ============================================================\n",
    "# PLOT ALEATORIC UNCERTAINTY\n",
    "# ============================================================\n",
    "print(\"\\nCreating aleatoric uncertainty visualization...\")\n",
    "_plot_uncertainty_pair(\n",
    "    y_test_true, mu_mean_test, aleatoric_test_std,\n",
    "    y_ood_true, mu_mean_ood, aleatoric_ood_std,\n",
    "    unc_label=\"Aleatoric\"\n",
    ")\n",
    "print(\"âœ“ Aleatoric uncertainty plot created\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Interpretation:\")\n",
    "print(\"   - Darker colors (yellow/green) = higher uncertainty\")\n",
    "print(\"   - Points far from diagonal = larger prediction errors\")\n",
    "print(\"   - OOD samples should show higher epistemic uncertainty (darker)\")\n",
    "print(\"   - Aleatoric uncertainty reflects irreducible noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d3dbfb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ Step 10: Conformal Prediction\n",
    "\n",
    "### 10.1 Introduction to Conformal Prediction\n",
    "\n",
    "**Conformal Prediction** provides prediction intervals with **distribution-free coverage guarantees**. Unlike probabilistic methods, it makes no assumptions about the data distribution.\n",
    "\n",
    "**Key Properties:**\n",
    "- **Finite-sample guarantees**: Coverage holds for any sample size\n",
    "- **Distribution-free**: No assumptions about data distribution\n",
    "- **Post-hoc method**: Can be applied to any trained model\n",
    "\n",
    "### 10.2 Training a Standard Regression MLP\n",
    "\n",
    "First, we'll train a standard (non-heteroscedastic) MLP for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417bb4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEFINE STANDARD REGRESSION MLP\n",
    "# ============================================================\n",
    "class RegressionMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard MLP for regression (single output).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    d_in : int\n",
    "        Input dimension\n",
    "    h : int\n",
    "        Hidden layer size\n",
    "    p : float\n",
    "        Dropout probability\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in, h=256, p=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, h), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(h, h//2), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(h//2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING FUNCTION\n",
    "# ============================================================\n",
    "def train_plain_mlp(X_train, y_train, X_val, y_val, *, \n",
    "                     lr=1e-3, wd=1e-5, bs=1024, \n",
    "                     epochs=500, patience=25, p_drop=0.1):\n",
    "    \"\"\"\n",
    "    Train a standard regression MLP with early stopping.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    RegressionMLP\n",
    "        Trained model with best validation loss\n",
    "    \"\"\"\n",
    "    d_in = X_train.shape[1]\n",
    "    model = RegressionMLP(d_in, p=p_drop).to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    crit = nn.MSELoss()\n",
    "\n",
    "    best_val = float('inf')\n",
    "    no_improve = 0\n",
    "    \n",
    "    print(\"ðŸš€ Starting training...\\n\")\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        perm = torch.randperm(X_train.size(0), device=DEVICE)\n",
    "        total = 0.0\n",
    "        \n",
    "        for i in range(0, X_train.size(0), bs):\n",
    "            idx = perm[i:i+bs]\n",
    "            xb, yb = X_train[idx], y_train[idx]\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            preds = model(xb)\n",
    "            loss = crit(preds, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item() * xb.size(0)\n",
    "        \n",
    "        train_loss = total / X_train.size(0)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = crit(model(X_val), y_val).item()\n",
    "\n",
    "        # Logging\n",
    "        if (ep + 1) % 10 == 0:\n",
    "            print(f\"Epoch {ep+1:03d}  train={train_loss:.4f}  val={val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(), \"plain_mlp_best.pt\")\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"\\nâ¹ï¸  Early stopping at epoch {ep+1}\")\n",
    "                break\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(\"plain_mlp_best.pt\", map_location=DEVICE))\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"\\nâœ“ Training complete! Best val loss: {best_val:.4f}\")\n",
    "    return model\n",
    "\n",
    "# ============================================================\n",
    "# PREDICTION HELPER FUNCTION\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def predict(model, X):\n",
    "    \"\"\"Get predictions from model.\"\"\"\n",
    "    model.eval()\n",
    "    return model(X.to(DEVICE)).squeeze(1).cpu().numpy()\n",
    "\n",
    "# ============================================================\n",
    "# TRAIN MODEL\n",
    "# ============================================================\n",
    "print(\"ðŸ—ï¸  Training Standard Regression MLP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "plain_model = train_plain_mlp(\n",
    "    train_X, train_y, val_X, val_y, \n",
    "    lr=1e-4, wd=1e-6, p_drop=0.2\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# GET PREDICTIONS\n",
    "# ============================================================\n",
    "print(\"\\nðŸ“Š Getting predictions on all splits...\")\n",
    "\n",
    "yhat_train = predict(plain_model, train_X)\n",
    "yhat_val = predict(plain_model, val_X)\n",
    "yhat_test = predict(plain_model, test_X)\n",
    "\n",
    "y_train_np = train_y.cpu().numpy().ravel()\n",
    "y_val_np = val_y.cpu().numpy().ravel()\n",
    "y_test_np = test_y.cpu().numpy().ravel()\n",
    "\n",
    "print(\"âœ“ Predictions complete\")\n",
    "\n",
    "# ============================================================\n",
    "# COMPUTE METRICS\n",
    "# ============================================================\n",
    "def regression_metrics(y_true, y_pred, name=\"Set\"):\n",
    "    \"\"\"Compute and print regression metrics.\"\"\"\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    pear = pearsonr(y_true, y_pred)[0]\n",
    "    spear = spearmanr(y_true, y_pred).correlation\n",
    "    print(f\"{name:<10s} | MSE={mse:.4f}  MAE={mae:.4f}  \"\n",
    "          f\"Pearson={pear:.3f}  Spearman={spear:.3f}\")\n",
    "    return dict(mse=mse, mae=mae, pearson=pear, spearman=spear)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Regression Performance:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_metrics = regression_metrics(y_train_np, yhat_train, \"Train\")\n",
    "val_metrics = regression_metrics(y_val_np, yhat_val, \"Val\")\n",
    "test_metrics = regression_metrics(y_test_np, yhat_test, \"Test\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# PLOT PREDICTIONS VS TRUE VALUES\n",
    "# ============================================================\n",
    "lims = [\n",
    "    min(y_test_np.min(), yhat_test.min()),\n",
    "    max(y_test_np.max(), yhat_test.max())\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(y_test_np, yhat_test, alpha=0.6, s=20, edgecolors=\"none\", color=\"royalblue\")\n",
    "plt.plot(lims, lims, 'r--', lw=2, label=\"Perfect prediction\")\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "plt.xlabel(\"True Target\", fontsize=12)\n",
    "plt.ylabel(\"Predicted\", fontsize=12)\n",
    "plt.title(\"Predicted vs True (Test Set)\", fontsize=13)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Baseline model ready for conformal prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8cf178",
   "metadata": {},
   "source": [
    "### 10.3 Split Conformal Calibration & Intervals\n",
    "\n",
    "We'll apply **Split Conformal Prediction** on top of a standard regression MLP to produce prediction intervals with finite-sample coverage guarantees.\n",
    "\n",
    "Process overview:\n",
    "1. Train a baseline regression model (already done)\n",
    "2. Use the validation set as the calibration set to compute residual quantiles\n",
    "3. Build intervals on test and OOD sets using the calibrated quantile\n",
    "4. Evaluate empirical coverage and average interval width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bd8c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFORMAL CALIBRATION FUNCTION\n",
    "# ============================================================\n",
    "def conformal_calibrate(y_cal, yhat_cal, alpha):\n",
    "    \"\"\"\n",
    "    Compute conformal quantile for split conformal prediction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_cal : numpy.ndarray\n",
    "        True values on calibration set\n",
    "    yhat_cal : numpy.ndarray\n",
    "        Predicted values on calibration set\n",
    "    alpha : float\n",
    "        Miscoverage level (e.g., 0.1 for 90% coverage)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    q : float\n",
    "        Conformal quantile\n",
    "    scores : numpy.ndarray\n",
    "        Nonconformity scores (absolute residuals)\n",
    "    \"\"\"\n",
    "    # Compute nonconformity scores (absolute residuals)\n",
    "    scores = np.abs(y_cal - yhat_cal)\n",
    "    n = scores.shape[0]\n",
    "    \n",
    "    # Finite-sample conservative correction\n",
    "    q_level = np.ceil((n + 1) * (1 - alpha)) / n\n",
    "    q = np.quantile(scores, q_level, method='higher')\n",
    "    \n",
    "    return q, scores\n",
    "\n",
    "# ============================================================\n",
    "# INTERVAL CONSTRUCTION\n",
    "# ============================================================\n",
    "def conformal_intervals(yhat, q):\n",
    "    \"\"\"\n",
    "    Construct prediction intervals using conformal quantile.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    lo, hi : tuple of numpy.ndarray\n",
    "        Lower and upper bounds of prediction intervals\n",
    "    \"\"\"\n",
    "    lo = yhat - q\n",
    "    hi = yhat + q\n",
    "    return lo, hi\n",
    "\n",
    "# ============================================================\n",
    "# COVERAGE METRICS\n",
    "# ============================================================\n",
    "def coverage_and_width(y_true, lo, hi):\n",
    "    \"\"\"\n",
    "    Compute empirical coverage and average interval width.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    cov : float\n",
    "        Empirical coverage (fraction of points within intervals)\n",
    "    width : float\n",
    "        Average interval width\n",
    "    \"\"\"\n",
    "    cov = np.mean((y_true >= lo) & (y_true <= hi))\n",
    "    width = np.mean(hi - lo)\n",
    "    return cov, width\n",
    "\n",
    "# ============================================================\n",
    "# PREPARE DATA FOR CONFORMAL PREDICTION\n",
    "# ============================================================\n",
    "print(\"ðŸ“ Setting up conformal prediction...\\n\")\n",
    "\n",
    "# Use validation set as calibration set\n",
    "yhat_cal = predict(plain_model, val_X)\n",
    "y_cal = val_y.cpu().numpy().ravel()\n",
    "\n",
    "# Define confidence levels\n",
    "alphas = [0.32, 0.10, 0.05]  # 68%, 90%, 95%\n",
    "confidence_levels = {0.32: \"68%\", 0.10: \"90%\", 0.05: \"95%\"}\n",
    "\n",
    "# Get predictions for test and OOD\n",
    "yhat_test = predict(plain_model, test_X)\n",
    "y_test_np = test_y.cpu().numpy().ravel()\n",
    "\n",
    "yhat_ood = predict(plain_model, ood_X)\n",
    "y_ood_np = ood_y.cpu().numpy().ravel()\n",
    "\n",
    "print(\"âœ“ Data prepared for conformal prediction\")\n",
    "\n",
    "# ============================================================\n",
    "# EVALUATE COVERAGE ACROSS CONFIDENCE LEVELS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Conformal Prediction Coverage Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for alpha in alphas:\n",
    "    conf_level = confidence_levels[alpha]\n",
    "    \n",
    "    # Calibrate on validation set\n",
    "    q, cal_scores = conformal_calibrate(y_cal, yhat_cal, alpha)\n",
    "    \n",
    "    # Test set intervals and metrics\n",
    "    lo_t, hi_t = conformal_intervals(yhat_test, q)\n",
    "    cov_t, w_t = coverage_and_width(y_test_np, lo_t, hi_t)\n",
    "    \n",
    "    print(f\"\\n{conf_level} Confidence (Î±={alpha}):\")\n",
    "    print(f\"  Quantile (q):        {q:.4f}\")\n",
    "    print(f\"  [Test]   Coverage:   {cov_t:.3f}  |  Mean width: {w_t:.3f}\")\n",
    "    \n",
    "    # OOD set intervals and metrics\n",
    "    lo_o, hi_o = conformal_intervals(yhat_ood, q)\n",
    "    cov_o, w_o = coverage_and_width(y_ood_np, lo_o, hi_o)\n",
    "    \n",
    "    print(f\"  [OOD]    Coverage:   {cov_o:.3f}  |  Mean width: {w_o:.3f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Observations:\")\n",
    "print(\"   - Coverage should match or exceed target confidence level\")\n",
    "print(\"   - OOD samples may have similar coverage (distribution-free property)\")\n",
    "print(\"   - Wider intervals = more conservative predictions\")\n",
    "\n",
    "# ============================================================\n",
    "# STORE RESULTS FOR VISUALIZATION\n",
    "# ============================================================\n",
    "# Choose one alpha to visualize (90% confidence)\n",
    "alpha_vis = 0.10\n",
    "q_vis, _ = conformal_calibrate(y_cal, yhat_cal, alpha_vis)\n",
    "\n",
    "# Compute intervals for visualization\n",
    "lo_t, hi_t = conformal_intervals(yhat_test, q_vis)\n",
    "lo_o, hi_o = conformal_intervals(yhat_ood, q_vis)\n",
    "\n",
    "print(f\"\\nâœ“ Prepared {confidence_levels[alpha_vis]} intervals for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d65547",
   "metadata": {},
   "source": [
    "### 10.4 Visualizing Prediction Intervals\n",
    "\n",
    "We'll visualize conformal intervals on the Test (in-distribution) and OOD splits, highlighting covered vs. not covered points. Shorter intervals with correct coverage indicate better uncertainty estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0c472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PLOTTING FUNCTION FOR CONFORMAL INTERVALS\n",
    "# ============================================================\n",
    "def plot_conformal_intervals(y_true, yhat, lo, hi, title, powt=None, max_points=120):\n",
    "    \"\"\"\n",
    "    Visualize conformal prediction intervals.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : numpy.ndarray\n",
    "        True target values\n",
    "    yhat : numpy.ndarray\n",
    "        Predicted values\n",
    "    lo, hi : numpy.ndarray\n",
    "        Lower and upper bounds of intervals\n",
    "    title : str\n",
    "        Plot title\n",
    "    powt : PowerTransformer, optional\n",
    "        If provided, inverse transform to original scale\n",
    "    max_points : int\n",
    "        Maximum number of points to plot (for readability)\n",
    "    \"\"\"\n",
    "    # Sort by true y and subsample for readability\n",
    "    idx = np.argsort(y_true)\n",
    "    y_true = y_true[idx]\n",
    "    yhat = yhat[idx]\n",
    "    lo = lo[idx]\n",
    "    hi = hi[idx]\n",
    "    step = max(1, len(y_true) // max_points)\n",
    "\n",
    "    # Optional: transform back to original scale\n",
    "    if powt is not None:\n",
    "        y_true = powt.inverse_transform(y_true.reshape(-1, 1)).ravel()\n",
    "        yhat = powt.inverse_transform(yhat.reshape(-1, 1)).ravel()\n",
    "        lo = powt.inverse_transform(lo.reshape(-1, 1)).ravel()\n",
    "        hi = powt.inverse_transform(hi.reshape(-1, 1)).ravel()\n",
    "\n",
    "    # Subsample for visualization\n",
    "    sel_idx = np.arange(0, len(y_true), step)\n",
    "\n",
    "    # Create masks for covered vs uncovered points\n",
    "    covered = (y_true >= lo) & (y_true <= hi)\n",
    "    sel_mask = np.zeros_like(covered, dtype=bool)\n",
    "    sel_mask[sel_idx] = True\n",
    "\n",
    "    # Intersection masks\n",
    "    mask_cov = covered & sel_mask\n",
    "    mask_un = (~covered) & sel_mask\n",
    "\n",
    "    # Error bars centered at yhat\n",
    "    yerr = np.vstack([yhat - lo, hi - yhat])\n",
    "\n",
    "    # Create plot\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    \n",
    "    # Covered points (blue)\n",
    "    if mask_cov.any():\n",
    "        plt.errorbar(\n",
    "            y_true[mask_cov], yhat[mask_cov],\n",
    "            yerr=yerr[:, mask_cov], fmt='o', ms=4, alpha=0.6,\n",
    "            ecolor='tab:blue', capsize=2, elinewidth=1, label='Covered'\n",
    "        )\n",
    "    \n",
    "    # Uncovered points (red)\n",
    "    if mask_un.any():\n",
    "        plt.errorbar(\n",
    "            y_true[mask_un], yhat[mask_un],\n",
    "            yerr=yerr[:, mask_un], fmt='o', ms=4, alpha=0.85,\n",
    "            ecolor='tab:red', capsize=2, elinewidth=1, label='Not covered'\n",
    "        )\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    lims = [\n",
    "        min(y_true.min(), yhat.min(), lo.min()),\n",
    "        max(y_true.max(), yhat.max(), hi.max())\n",
    "    ]\n",
    "    plt.plot(lims, lims, 'k--', lw=2, label='Perfect prediction', alpha=0.5)\n",
    "    \n",
    "    plt.xlim(lims)\n",
    "    plt.ylim(lims)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.xlabel(\"True Target\", fontsize=12)\n",
    "    plt.ylabel(\"Predicted Â± Interval\", fontsize=12)\n",
    "    plt.title(title, fontsize=13)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZE TEST SET INTERVALS\n",
    "# ============================================================\n",
    "print(\"\\nðŸ“Š Creating conformal interval visualizations...\\n\")\n",
    "\n",
    "plot_conformal_intervals(\n",
    "    y_test_np, yhat_test, lo_t, hi_t,\n",
    "    title=f\"Conformal Intervals on Test Set (1-Î±={1-alpha_vis:.2f})\"\n",
    ")\n",
    "print(\"âœ“ Test set visualization complete\")\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZE OOD SET INTERVALS\n",
    "# ============================================================\n",
    "plot_conformal_intervals(\n",
    "    y_ood_np, yhat_ood, lo_o, hi_o,\n",
    "    title=f\"Conformal Intervals on OOD Set (1-Î±={1-alpha_vis:.2f})\"\n",
    ")\n",
    "print(\"âœ“ OOD set visualization complete\")\n",
    "\n",
    "# ============================================================\n",
    "# OPTIONAL: PLOT IN ORIGINAL TEMPERATURE SCALE\n",
    "# ============================================================\n",
    "# Uncomment to visualize in original temperature units (Â°C)\n",
    "# print(\"\\nðŸ“Š Creating plots in original temperature scale...\\n\")\n",
    "# \n",
    "# plot_conformal_intervals(\n",
    "#     y_test_np, yhat_test, lo_t, hi_t, \n",
    "#     powt=powt,\n",
    "#     title=f\"Conformal Intervals on Test, Original Scale (Â°C) (1-Î±={1-alpha_vis:.2f})\"\n",
    "# )\n",
    "# \n",
    "# plot_conformal_intervals(\n",
    "#     y_ood_np, yhat_ood, lo_o, hi_o, \n",
    "#     powt=powt,\n",
    "#     title=f\"Conformal Intervals on OOD, Original Scale (Â°C) (1-Î±={1-alpha_vis:.2f})\"\n",
    "# )\n",
    "\n",
    "print(\"\\nðŸ’¡ Interpretation:\")\n",
    "print(\"   - Blue points: True value falls within prediction interval\")\n",
    "print(\"   - Red points: True value outside interval (miscoverage)\")\n",
    "print(\"   - Error bars show interval width (yhat Â± q)\")\n",
    "print(\"   - Ideally: High coverage, narrow intervals, points near diagonal\")\n",
    "\n",
    "print(\"\\nâœ“ Conformal prediction analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ebfa0b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Step 11: Wrap-up and Next Steps\n",
    "\n",
    "### What we covered\n",
    "- Built a robust pipeline for Uncertainty Quantification in protein ML\n",
    "- Used frozen ESM2 embeddings and trained an MLP classifier for EC prediction\n",
    "- Assessed calibration and improved it via Temperature Scaling\n",
    "- Modeled heteroscedastic regression with MC Dropout to separate epistemic and aleatoric uncertainty\n",
    "- Applied Split Conformal Prediction to obtain intervals with finite-sample coverage guarantees\n",
    "\n",
    "### Key takeaways\n",
    "- Calibration matters: temperature scaling can significantly reduce ECE\n",
    "- Epistemic vs Aleatoric: MC Dropout helps disentangle model and data uncertainty\n",
    "- Conformal methods provide reliable coverage without strong distributional assumptions\n",
    "\n",
    "### Try next\n",
    "- Swap to a larger ESM2 model (e.g., esm2_t12_35M_UR50D) for richer embeddings\n",
    "- Explore different binning or ECE variants (adaptive bins) for calibration analysis\n",
    "- Use class-conditional temperature scaling or vector scaling for multi-class calibration\n",
    "- Experiment with other UQ methods (deep ensembles, SWAG, Laplace)\n",
    "\n",
    "### Reproducibility notes\n",
    "- Random seeds are not fixed across all components; for exact reproducibility, set seeds in NumPy, PyTorch, and scikit-learn\n",
    "- GPU precision set to float16 for ESM2 embeddings; adjust to float32 if needed for CPUs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
